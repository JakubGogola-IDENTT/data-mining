{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "from string import punctuation\n",
    "from itertools import groupby\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/stopwords.txt', encoding='UTF-8') as f:\n",
    "    stopwords = [word for line in f for word in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(words, stopwords):\n",
    "    filtered_words = [word.lower().translate(str.maketrans('','', punctuation)) for word in words]\n",
    "    filtered_words = [word for word in filtered_words if word not in stopwords] # add stemming\n",
    "    return filtered_words\n",
    "\n",
    "def count_words(words):\n",
    "    def add_to_dict(acc, word):\n",
    "        if word not in acc or not word:\n",
    "            acc[word] = 1\n",
    "        else:\n",
    "            acc[word] += 1\n",
    "\n",
    "        return acc\n",
    "\n",
    "    words_count = reduce(lambda acc, word: add_to_dict(acc, word), words, {})\n",
    "    words_count = [(word, count) for word, count in words_count.items()]\n",
    "    words_count.sort(key=(lambda pair: pair[1]), reverse=True)\n",
    "    \n",
    "    return words_count \n",
    "\n",
    "def to_csv(file_dir, words_count):\n",
    "    with open(file_dir, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for word, count in words_count:\n",
    "            writer.writerow([count, word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cobc.txt', encoding='UTF-8') as f:\n",
    "    words_list = [word for line in f for word in line.split()]\n",
    "\n",
    "filtered_words = process_words(words_list, stopwords)\n",
    "words_count = count_words(filtered_words)\n",
    "to_csv('./results/words_count.csv', words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(term, doc):\n",
    "    return doc.count(term)\n",
    "\n",
    "def idf(term, docs):\n",
    "    docs_count = len(docs)\n",
    "    \n",
    "    def count_occurrences(acc, doc):\n",
    "        if doc.count(term) > 0:\n",
    "            acc += 1\n",
    "        \n",
    "        return acc\n",
    "    \n",
    "    occurrences = reduce(lambda acc, doc: count_occurrences(acc, doc), docs, 0)\n",
    "    \n",
    "    return math.log(docs_count / (1.0 + occurrences))\n",
    "\n",
    "def tfidf(term, doc, docs):\n",
    "    return tf(term, doc) * idf(term, docs)\n",
    "\n",
    "def to_sorted_tuples(data):\n",
    "    tuples_list = [(key, value) for key, value in data.items()]\n",
    "    tuples_list.sort(key=(lambda pair: pair[1]), reverse=True)\n",
    "    \n",
    "    return tuples_list\n",
    "\n",
    "def compute_tfidfs(doc, docs):\n",
    "    def add_to_dict(acc, term):\n",
    "        acc[term] = tfidf(term, doc, docs)\n",
    "        \n",
    "        return acc\n",
    "    \n",
    "    tfidfs = reduce(lambda acc, term: add_to_dict(acc, term), doc, {})\n",
    "    \n",
    "    return tfidfs\n",
    "\n",
    "def compute_tfidfs_for_docs(docs):\n",
    "    return [compute_tfidfs(doc, docs) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cobc.txt', encoding='UTF-8') as f:\n",
    "    chapters = []\n",
    "    for key, group in groupby(f, lambda line: line.startswith('Chapter')):\n",
    "        if not key:\n",
    "            group = list(group)\n",
    "            chapter = [word for line in group for word in line.split()]\n",
    "            chapters.append(chapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = [process_words(chapter, stopwords) for chapter in chapters]\n",
    "chapters = chapters[0:15] # TODO: remove it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters_tfidfs = compute_tfidfs_for_docs(chapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, tfidfs in enumerate(chapters_tfidfs):\n",
    "    tupled = to_sorted_tuples(tfidfs)\n",
    "    tupled = [(word, int(count * 1000)) for word, count in tupled if word and count > 0]\n",
    "    \n",
    "    file_dir = './results/tfidfs_ch_{no}.csv'.format(no=idx)\n",
    "    to_csv(file_dir, tupled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = {}\n",
    "\n",
    "for tfidfs in chapters_tfidfs:\n",
    "    tupled = to_sorted_tuples(tfidfs)\n",
    "    \n",
    "    def add_to_dict(acc, pair):\n",
    "        if pair[0] not in acc:\n",
    "            acc[pair[0]] = pair[1]\n",
    "        else:\n",
    "            acc[pair[0]] += pair[1]\n",
    "\n",
    "        return acc\n",
    "    \n",
    "    reduce(lambda acc, pair: add_to_dict(acc, pair), tupled, merged)\n",
    "    \n",
    "tupled = [(word, int(count * 100)) for word, count in to_sorted_tuples(merged) if word and count > 0]\n",
    "to_csv('./results/tfidfs.csv', tupled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_chapters(term, docs):\n",
    "    docs_tfidfs = compute_tfidfs_for_docs(docs)\n",
    "    enumerated = [(idx, tfidfs) for idx, tfidfs in enumerate(docs_tfidfs)]\n",
    "        \n",
    "    def get_word_tfidf(tfidfs):\n",
    "        if term in tfidfs:\n",
    "            return tfidfs[term]\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    enumerated.sort(key=(lambda pair: get_word_tfidf(pair[1])), reverse=True)\n",
    "    \n",
    "    return [no for no, _ in enumerated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 3, 12, 11, 13, 1, 5, 7, 9, 0, 2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_chapters('juniper', chapters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercies 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_successors(words):\n",
    "    words_set = set(words)\n",
    "    \n",
    "    successors = {}\n",
    "    \n",
    "    for i, word in enumerate(words_set):\n",
    "        if not word:\n",
    "            continue\n",
    "        \n",
    "        word_successors = {}\n",
    "        \n",
    "        for idx, w in enumerate(words):\n",
    "            if word and w == word and (idx + 1) < len(words):\n",
    "                successor = words[idx + 1]\n",
    "                \n",
    "                if successor not in word_successors:\n",
    "                    word_successors[successor] = 1\n",
    "                else:\n",
    "                    word_successors[successor] += 1\n",
    "        \n",
    "        sorted_word_succesors = to_sorted_tuples(word_successors)[:5]\n",
    "        successors[word] = [sc for sc, _ in sorted_word_succesors]\n",
    "    \n",
    "    return successors\n",
    "\n",
    "def create_random_paragraph(words, successors):\n",
    "    successors = [(word, successors_list) for word, successors_list in successors.items()]\n",
    "    \n",
    "    paragraph_len = random.randint(50, 150)\n",
    "    \n",
    "    paragraph = ''\n",
    "    \n",
    "    for i in range(paragraph_len):\n",
    "        sentence_len = random.randint(3, 10)\n",
    "        \n",
    "        sentence = ''\n",
    "        for j in range(sentence_len):\n",
    "            word_idx = random.randint(0, len(successors) - 1) \n",
    "            word = successors[word_idx][0]\n",
    "            \n",
    "            successors_list = successors[word_idx][1]\n",
    "            successor_idx = random.randint(0, len(successors_list) - 1)\n",
    "            successor = successors_list[successor_idx]\n",
    "            \n",
    "            sentence += '{word} {successor} '.format(word=word, successor=successor)\n",
    "        \n",
    "        paragraph += sentence.strip().capitalize()\n",
    "        paragraph += '. '\n",
    "        \n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "succesors = get_successors(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/random_paragraph.txt', 'w') as f:\n",
    "    paragraph = create_random_paragraph(filtered_words, succesors)\n",
    "    f.write(paragraph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
